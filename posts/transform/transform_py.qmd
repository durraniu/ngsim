---
title: "Transform Data with Python"
date: "2024-01-21"
categories: [import, transform, pandas, polars, janitor]
image: "https://media.giphy.com/media/xUPGcAx3wr7jszFfpe/giphy.gif"
code-annotations: hover
jupyter: python3
---

::: {.callout-note}
This is the second post in the NGSIM data analysis series. Previous post:   

* [Import data](https://itav-ngsim.netlify.app/posts/import/import)  

:::


::: {.callout-tip}
## After completing this post, you will be able to:

* Read parquet files  
* Do a quick clean-up of column names    
* Convert integer time to datetime fromat  
* Sort data by columns  
* Create new columns  
* Remove undesired columns  
* Filter data  
    
:::

[This post shows code in `Python`. You can see the same post written in `R` [here](transform_r.qmd).]{.aside}

In the previous post, we partitioned the Interstate 80 (I80) vehicle trajectories data by time and then saved it on disk:  

![](i80_period.PNG)  

We now make use of the parquet file in the `period=first` directory for learning to transform data. Since this file is part of partitioned data, I saved it as a separate parquet file (`i80_period1.parquet`) in a different location.    

[You may read the partitioned parquet file also, but R and Python load it slightly differently.]{.aside} 

Now, we read the `i80_period1.parquet` file:  

## Load parquet file 

Reading the parquet file requires the `polars` package:  

```{python pyread}
import polars as pl

path_to_first_period_file = "data/i80_period1.parquet"
data_py = pl.read_parquet(path_to_first_period_file)

data_py.head()
```




## Clean dataframe names

As you can see above, the column names are in good shape i.e., without any spaces. However, it is easier typing lowercase letters than the sentence case. So, we use the `clean_names` function from the `janitor` package to make all column names lowercase. If the column names have spaces or periods in them, `clean_names` would replace them with underscores.   

Using `pyjanitor` requires the data to be a `pandas` dataframe. But we loaded the data from a parquet file using the `polars` package, so we need to first convert the `polars` dataframe to a pandas dataframe. This further requires importing the `pyarrow` package:  

```{python pyclean}
# Installation:
# pip install pyjanitor
# pip install pyarrow
# pip install pandas

import pandas as pd
import pyarrow
import janitor
data_py = data_py.to_pandas()
data_py = data_py.clean_names()

data_py.head()
```


## How to create a time column?

Since vehicle trajectories change over time, it is nice to see these changes over different time periods. However, the `gloabl_time` column in this dataset contains integers rather than the actual time. So, we create a new column called `actual_time` by dividing the `global_time` by 1000 and converting it to a datetime object.  

In Python, this can be done via `pandas.to_datetime`. Note that we specify the time zone to be `America/Los_Angeles`.  

```{python pytime1}
data_py['actual_time'] = pd.to_datetime(data_py['global_time'] / 1000, 
                                    unit='s', origin='1970-01-01', utc=True)
data_py['actual_time'] = data_py['actual_time'].dt.tz_convert('America/Los_Angeles')

data_py.head()
```

Note that the data is not in the correct order. The `vehicle_id`s in the first two rows are 3027 and 3214. However, we know that the same vehicle was observed for several seconds. This means that we should see a given `vehicle_id` repeated over multiple rows consecutively. We therefore sort by `vehicle_id` and `frame_id`:  


```{python pytime2}
## First: Sort by Vehicle ID and Time
data_py = data_py.sort_values(by = ["vehicle_id", "frame_id"])

data_py.head()
```

  

When we want to compare several vehicle trajectories, e.g., how their speeds change over time regardless of when they were observed, we'd want a common time scale. The NGSIM documentation describes that vehicles were observed at a resolution of 0.1 seconds. So, we can create a`time` variable for each vehicle that starts at 0 and ends at (n-1)/10 where n = number of rows for which a `vehicle_id` is repeated. 

We first define a function `calculate_time_elapsed` that takes in a dataframe and returns the sequence from 0 to (n-1)/10 with a step size of 0.1 as a new column. Then we apply this function on the `pandas` dataframe:  

```{python pytime3}
def calculate_time_elapsed(group_df):
    num_rows = len(group_df)
    group_df['time'] = [i / 10.0 for i in range(num_rows)]
    return group_df

# Add the time elapsed column to the DataFrame within each group
data_py = data_py.groupby('vehicle_id').apply(calculate_time_elapsed)

data_py.head()
```



## How to create variables for the preceding vehicle?

We'd often need velocity, acceleration, and other variables for the preceding vehicle (vehicle in front of the subject vehicle). These variables can be useful in car-following modeling.   

In this dataset, preceding vehicle variables do not exist as separate columns. The only relevant column is `preceding` which is the identifier of the preceding vehicle. But the data also contains the subject vehicle identifier `vehicle_id`, along with these variables:  

* `local_y`: longitudinal position of the front end of the subject vehicle (feet)     
* `local_x`: lateral position of the front end of the subject vehicle (feet)  
* `v_length` and `v_width` are the length and width of the subject vehicle (feet)  
* `v_class` is the class of the subject vehicle, i.e., 1 = motorcycle, 2 = car, and 3 = heavy vehicle (bus/truck)  
* `v_vel` and `v_acc` are the velocity (ft/s) and acceleration (ft/s/s) of the subject vehicle  

Our goal now is to create new columns of the above variables for the preceding vehicle. To this end, we look for the value of `preceding` in the `vehicle_id` column at a given value of `frame_id` (identifier of time frame) and then grab the value of variable e.g., `v_vel` at that `frame_id`. In Python, we achieve this by joining a few columns of the dataset with the full dataset while using the columns `vehicle_id` and `preceding` for the join:  

```{python}
data_py = data_py.reset_index(drop=True)

## Create new cols
data_py = data_py.merge(    # <1>
  data_py.loc[:, ['frame_id', 'vehicle_id', 'local_x', 'local_y', 'v_length',
            'v_width', 'v_class', 'v_vel', 'v_acc']] ,    # <2>
              left_on = ['frame_id', 'preceding'],  # <3>
              right_on = ['frame_id', 'vehicle_id'],  # <3>
              how = 'left',  # <4> 
              suffixes=['', '_preceding'] # <5>
              )
data_py = data_py.drop(['vehicle_id_preceding'], axis = 'columns') # <6>

data_py.head()
```
1. `merge` is used for joining `pandas` dataframes.  
2. This is part of the `data_py` dataframe. It includes those variables of the subject vehicle that we want to create for the preceding vehicle.  
3.  `data_py` is the "left" dataframe and `data_py.loc[:, ['frame_id', 'vehicle_id', 'local_x', 'local_y', 'v_length', 'v_width', 'v_class', 'v_vel', 'v_acc']]` is the "right" dataframe. At a given `frame_id`, 'local_x', 'local_y', 'v_length', 'v_width', 'v_class', 'v_vel', 'v_acc' are joined with `data_py` using the `preceding` and `vehicle_id` columns.   
4. The type of join is "left".  
5. A suffix `_preceding` is added to show that these variables are for the preceding vehicle.  
6. This operation created a redundant preceding vehicle ID that we drop here.    
        

A `NaN` or `null` value indicates missing value. In this dataset, `NaN` / `null` indicates that the value is missing because there was no preceding vehicle observed. For `vehicle_id` 1 we can see this is true because the `preceding` value is 0. 


To keep the column names consistent with the result in the same post with `R` code, we rename the preceding vehicle variables:  

```{python}
data_py = pl.from_pandas(data_py) # <1> 

data_py = data_py.rename({  # <2>
    "local_y_preceding":"preceding_local_y", 
    "v_length_preceding":"preceding_length", 
    "v_width_preceding":"preceding_width", 
    "v_class_preceding":"preceding_class", 
    "v_vel_preceding":"preceding_vel", 
    "v_acc_preceding":"preceding_acc"
    })
    
data_py.columns
```
1. Convert the `pandas` dataframe to `polars` dataframe. 
2. Use the `polars.rename` function to rename the columns.  



## How to remove undesired columns?

There are several variables in this dataset that we don't need as they are completely devoid of any value. So we remove them: 

```{python}
data_py = data_py.drop(["total_frames", "global_x", "global_y", "following", 
              "o_zone", "d_zone", "int_id", "section_id", "direction", 
             "movement"])
             
data_py.head()
```



## How to transform multiple columns?

### Metric units

![](https://media.giphy.com/media/xUPGcAx3wr7jszFfpe/giphy.gif)  

As discussed before, variables in this dataset have imperial units (feet, ft/s, etc.). You may want to transform the values of these variables to metric units. The conversion factor is 0.3048. Here, we utilize the `polars.with_columns` function to take all the desired columns (`cols_to_convert_to_metric`) and apply the conversion factor along with rounding to 2 decimal places:  

```{python}
## convert to metric
cols_to_convert_to_metric = ['local_x', 'local_y', 'v_length', 'v_width', 
        'v_vel', 'v_acc', 'space_headway', 'preceding_local_y',
        'preceding_length', 'preceding_width', 'preceding_vel',
       'preceding_acc']

data_py = data_py.with_columns((pl.col(cols_to_convert_to_metric) * .3048).round(2))

data_py.head()
```

### Convert numbers/strings to categorical data type

Moreover, we know that there are variables that should be treated as categorical (qualitative) rather than numbers or strings. For instance, `lane_id` has values 1-7 and we know that these are identifiers for lanes. Similarly, the class of a vehicle is encoded as 1, 2, and 3 but we know that these numbers do not have any quantitaive information, rather they are categories.   

In `polars`, categorical data is encoded as `polars.Categorical` data type:   

```{python}
## change the data type to categorical
cols_to_convert_to_categorical = ['vehicle_id', 'v_class', 'lane_id', 
                             'preceding', 'preceding_class']
data_py = data_py.with_columns(pl.col(cols_to_convert_to_categorical).cast(pl.String).cast(pl.Categorical)) # <1>

data_py.head()
```
1. Columns are first converted to string data type and then the strings are converted to categorical data type.  




## Visualization with one vehicle

Cool! We are almost done with transforming our dataset. It is time to do some visualization. The last transformation we learn is to filter the data to keep only one vehicle:  

```{python}
data_py_veh = data_py.filter(pl.col('vehicle_id') == "2")
```

And now we use `ggplot2` to create a plot of velocity over time. Subject vehicle in blue and preceding vehicle in orange. 

```{python}
from lets_plot import *
LetsPlot.setup_html()
(
ggplot(data = data_py_veh) +\
  geom_path(mapping = aes(x = 'time', y = 'v_vel'), color = 'blue') +\
  geom_path(mapping = aes(x = 'time', y = 'preceding_vel'), color = 'orange') +\
  labs(x = "Time (s)", y = "Velocity (m/s)",
       title = "Velocity of vehicle # 2 and its preceding vehicle") +\
  theme_minimal()
)
```